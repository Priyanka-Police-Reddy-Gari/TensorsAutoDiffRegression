{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyanka-Police-Reddy-Gari/TensorsAutoDiffRegression/blob/main/Auto_Diff_regression_priyanka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c32ad0f3-a41b-477a-ad23-105010ed39e1",
      "metadata": {
        "id": "c32ad0f3-a41b-477a-ad23-105010ed39e1"
      },
      "source": [
        "# <font color = 'indianred'>**Task 1 - Autodiff**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w2uzS2n43CsD"
      },
      "id": "w2uzS2n43CsD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IsWkg1CufBwg",
      "metadata": {
        "id": "IsWkg1CufBwg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uSj20OEuf6bf",
      "metadata": {
        "id": "uSj20OEuf6bf"
      },
      "source": [
        "##  <font color = 'indianred'>**Normalize Function**<font>\n",
        "\n",
        "Wrote the function that normalizes the columns of a matrix. To compute the mean and standard deviation of each column. Then for each element of the column,  subtract the mean and divide by the standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8L9JBFNilkWt",
      "metadata": {
        "id": "8L9JBFNilkWt"
      },
      "outputs": [],
      "source": [
        "# Given Data\n",
        "x = [[ 3,  60,  100, -100],\n",
        "     [ 2,  20,  600, -600],\n",
        "     [-5,  50,  900, -900]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iRrhopVBl-0q",
      "metadata": {
        "id": "iRrhopVBl-0q"
      },
      "outputs": [],
      "source": [
        "# Convert to PyTorch Tensor and set to float\n",
        "X = torch.tensor(x)\n",
        "X= X.float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S2MaocHxmEQJ",
      "metadata": {
        "id": "S2MaocHxmEQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dd01f75-967d-4a83-992b-43416b3c3e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.float32\n"
          ]
        }
      ],
      "source": [
        "# Print shape and data type for verification\n",
        "print(X.shape)\n",
        "print(X.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rPgb1L9RmQAU",
      "metadata": {
        "id": "rPgb1L9RmQAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2f44a9-4b2c-4764-df1a-02883d234866"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  4.3589,  20.8167, 404.1452, 404.1452])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# Compute and display the mean and standard deviation of each column for reference\n",
        "X.mean(axis = 0)\n",
        "X.std(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qnM87Db1mqFH",
      "metadata": {
        "id": "qnM87Db1mqFH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a2b493-a2df-4a18-a899-4d9a5b8b8a50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  4.3589,  20.8167, 404.1452, 404.1452])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "X.std(axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- task starts here\n",
        "- normalize_matrix function takes a PyTorch tensor x as input.\n",
        "- It returns a tensor where the columns are normalized.\n",
        "- After implementing function, used the code provided to verify if the mean for each column in Z is close to zero and the standard deviation is 1."
      ],
      "metadata": {
        "id": "Yy2hehDnJHOq"
      },
      "id": "Yy2hehDnJHOq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mwq8qnqFlu9V",
      "metadata": {
        "id": "mwq8qnqFlu9V"
      },
      "outputs": [],
      "source": [
        "def normalize_matrix(x):\n",
        "  # Calculated the mean along each column (think carefully , you will take mean along axis = 0 or 1)\n",
        "  mean = x.mean(axis=0)\n",
        "  # Calculated the standard deviation along each column\n",
        "  std = x.std(axis=0)\n",
        "\n",
        "  # Normalized each element in the columns by subtracting the mean and dividing by the standard deviation\n",
        "  y = (x - mean) / std;\n",
        "\n",
        "  return y  # Returned the normalized matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m027Qcgwm9OL",
      "metadata": {
        "id": "m027Qcgwm9OL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "925b3b8f-ebdd-4e4b-e16e-7c416de199c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6882,  0.8006, -1.0722,  1.0722],\n",
              "        [ 0.4588, -1.1209,  0.1650, -0.1650],\n",
              "        [-1.1471,  0.3203,  0.9073, -0.9073]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "Z = normalize_matrix(X)\n",
        "Z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78-0D3KfnHel",
      "metadata": {
        "id": "78-0D3KfnHel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2d7c66c-5b4e-433c-9608-8df0a2a8e13e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0000e+00,  4.9671e-08,  3.9736e-08, -3.9736e-08])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "Z.mean(axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NETkPmz8n142",
      "metadata": {
        "id": "NETkPmz8n142",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d4253d7-7558-452e-96f2-f13eb0128c35"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "Z.std(axis = 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2816679-bf3f-4f97-82b7-3b1f9780b609",
      "metadata": {
        "id": "b2816679-bf3f-4f97-82b7-3b1f9780b609"
      },
      "source": [
        "##  <font color = 'indianred'>**Calculate Gradients**\n",
        "\n",
        "Computed Gradient using  PyTorch Autograd\n",
        "## $f(x,y) = \\frac{x + \\exp(y)}{\\log(x) + (x-y)^3}$\n",
        "Computed dx and dy at x=3 and y=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rcDTwteKbZUH",
      "metadata": {
        "id": "rcDTwteKbZUH"
      },
      "outputs": [],
      "source": [
        "def fxy(x, y):\n",
        "  # Calculated the numerator: Add x to the exponential of y\n",
        "  num = x + torch.exp(y)\n",
        "\n",
        "  # Calculated the denominator: Sum of the logarithm of x and cube of the difference between x and y\n",
        "  den = torch.log(x) + (x - y)**3\n",
        "\n",
        "  # Performed element-wise division of the numerator by the denominator\n",
        "  return (num/den)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ADHy2FqfP8S",
      "metadata": {
        "id": "0ADHy2FqfP8S"
      },
      "outputs": [],
      "source": [
        "# Created a single-element tensor 'x' containing the value 3.0\n",
        "# made sure to set 'requires_grad=True' as you want to compute gradients with respect to this tensor during backpropagation\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# Created a single-element tensor 'y' containing the value 4.0\n",
        "# Similar to 'x', we wanted to compute gradients for 'y' during backpropagation, hence make sure to set 'requires_grad=True'\n",
        "y = torch.tensor(4.0,requires_grad=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_2HFXif6fg7A",
      "metadata": {
        "id": "_2HFXif6fg7A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea8862a-306b-410b-eab4-2d9bf478bd0f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(584.0868, grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "# Called the function 'fxy' with the tensors 'x' and 'y' as arguments\n",
        "# The result 'f' will also be a tensor and contained derivative information because 'x' and 'y' have 'requires_grad=True'\n",
        "f = fxy(x, y)\n",
        "f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TbZiDeTXfrLf",
      "metadata": {
        "id": "TbZiDeTXfrLf"
      },
      "outputs": [],
      "source": [
        "# Performed backpropagation to compute the gradients of 'f' with respect to 'x' and 'y'\n",
        "# used backward() function on f\n",
        "\n",
        "f.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7al8BVZKf3R7",
      "metadata": {
        "id": "7al8BVZKf3R7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba0a6d02-4b35-4a6b-ca7e-c2fc528be22e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad = tensor(-19733.3965)\n",
            "y.grad = tensor(18322.8477)\n"
          ]
        }
      ],
      "source": [
        "# Displayed the computed gradients of 'f' with respect to 'x' and 'y'\n",
        "# These gradients are stored as attributes of x and y after the backward operation\n",
        "# Printed the gradients for x and y\n",
        "print('x.grad =', x.grad)\n",
        "print('y.grad =', y.grad)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX5yKE-t0sEn"
      },
      "source": [
        "## <font color = 'indianred'>**Numerical Precision**\n",
        "\n",
        "Given scalars `x` and `y`, implemented the following `log_exp` function such that it returns\n",
        "$$-\\log\\left(\\frac{e^x}{e^x+e^y}\\right)$$."
      ],
      "id": "RX5yKE-t0sEn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.206890Z",
          "start_time": "2019-01-29T22:48:56.202996Z"
        },
        "id": "nikSfenDC3c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def log_exp(x, y):\n",
        "    ## add your solution here and remove pass\n",
        "    return -torch.log(torch.exp(x)/(torch.exp(x)+torch.exp(y)))\n",
        "\n",
        "\n"
      ],
      "id": "nikSfenDC3c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ada0LKKYC3c3"
      },
      "source": [
        "Tested codes with normal inputs:"
      ],
      "id": "ada0LKKYC3c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.215579Z",
          "start_time": "2019-01-29T22:48:56.209659Z"
        },
        "id": "oIZ0UETBC3c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a581362e-783a-4df8-9d19-fc25a04847a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.3133])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "# Created tensors x and y with initial values 2.0 and 3.0, respectively\n",
        "x, y = torch.tensor([2.0]), torch.tensor([3.0])\n",
        "\n",
        "# Evaluated the function log_exp() for the given x and y, and store the output in z\n",
        "z = log_exp(x, y)\n",
        "\n",
        "# Displayed the computed value of z\n",
        "z\n"
      ],
      "id": "oIZ0UETBC3c3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbGtCaQ6C3c4"
      },
      "source": [
        "Now implemented a function to compute $\\partial z/\\partial x$ and $\\partial z/\\partial y$ with `autograd`"
      ],
      "id": "sbGtCaQ6C3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.223303Z",
          "start_time": "2019-01-29T22:48:56.218056Z"
        },
        "id": "2kMC5IdJC3c4"
      },
      "outputs": [],
      "source": [
        "def grad(forward_func, x, y):\n",
        "  # Enable gradient tracking for x and y, set reauires_grad appropraitely\n",
        "  # x, y = x.requires_grad_(True), y.requires_grad_(True)\n",
        "  # CODE HERE\n",
        "  x.requires_grad_(True), y.requires_grad_(True)\n",
        "\n",
        "  # Evaluate the forward function to get the output 'z'\n",
        "  z = forward_func(x, y)\n",
        "\n",
        "  # Perform the backward pass to compute gradients\n",
        "  # Hint use backward() function on z\n",
        "  z.backward()\n",
        "\n",
        "  # Print the gradients for x and y\n",
        "  print('x.grad =', x.grad)\n",
        "  print('y.grad =', y.grad)\n",
        "\n",
        "  # Reset the gradients for x and y to zero for the next iteration\n",
        "  x.grad.zero_()\n",
        "  y.grad.zero_()\n",
        "\n",
        "\n"
      ],
      "id": "2kMC5IdJC3c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Q3d5opC3c4"
      },
      "source": [
        "Tested codes, it printed the results nicely."
      ],
      "id": "g5Q3d5opC3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.267165Z",
          "start_time": "2019-01-29T22:48:56.227035Z"
        },
        "id": "oob8ND3BC3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56dbdcb0-2835-4de1-f4c2-33196ca43185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad = tensor([-0.7311])\n",
            "y.grad = tensor([0.7311])\n"
          ]
        }
      ],
      "source": [
        "grad(log_exp, x, y)"
      ],
      "id": "oob8ND3BC3c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4nMM2joC3c4"
      },
      "source": [
        "But now let's try some \"hard\" inputs"
      ],
      "id": "L4nMM2joC3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.285842Z",
          "start_time": "2019-01-29T22:48:56.274079Z"
        },
        "id": "d_f5Xk41C3c4"
      },
      "outputs": [],
      "source": [
        "x, y = torch.tensor([50.0]), torch.tensor([100.0])"
      ],
      "id": "d_f5Xk41C3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECgqQ_i-C3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75bd5e66-b23b-44f4-b94c-2b7adba16b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad = tensor([nan])\n",
            "y.grad = tensor([nan])\n"
          ]
        }
      ],
      "source": [
        "# you may see nan/inf values as output, this is not an error\n",
        "grad(log_exp, x, y)"
      ],
      "id": "ECgqQ_i-C3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsyDVblC3c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc39ec7e-ac73-473e-d836-d2a69271a858"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([inf])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# you may see nan/inf values as output, this is not an error\n",
        "torch.exp(torch.tensor([100.0]))"
      ],
      "id": "kHsyDVblC3c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clK0zCMjC3c4"
      },
      "source": [
        "Does the code return correct results? If not, we try to understand the reason. (Hint, evaluate `exp(100)`). Now developed a new function `stable_log_exp` that is identical to `log_exp` in math, but returns a more numerical stable result.\n",
        "<br> Hint: (1) $\\log\\left(\\frac{x}{y}\\right) = log ({x}) -log({y})$\n",
        "<br> Hint: (2) See logsum Trick - https://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/"
      ],
      "id": "clK0zCMjC3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-29T22:48:56.305595Z",
          "start_time": "2019-01-29T22:48:56.293399Z"
        },
        "id": "alymet8aC3c4"
      },
      "outputs": [],
      "source": [
        "def stable_log_exp(x, y):\n",
        "    max = torch.max(x,y)\n",
        "    return(-(x - (max + torch.log(torch.exp(x-max)+ torch.exp(y-max)))))\n",
        "\n"
      ],
      "id": "alymet8aC3c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsrgvsFh8UFq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d7052a-1b89-4a25-f1d3-86730ccc3d3b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([inf], grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "log_exp(x, y)"
      ],
      "id": "ZsrgvsFh8UFq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NoH9aa48Lzf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae021b2f-45db-45c1-a27c-fc8bbbd0cf56"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([50.], grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "stable_log_exp(x, y)"
      ],
      "id": "3NoH9aa48Lzf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1uQXi8AC3c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e64bd0-56d4-466e-ebde-aa01685ec642"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad = tensor([-1.])\n",
            "y.grad = tensor([1.])\n"
          ]
        }
      ],
      "source": [
        "grad(stable_log_exp, x, y)"
      ],
      "id": "d1uQXi8AC3c5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'indianred'>**Task 2 - Linear Regression using Batch Gradient Descent with PyTorch**"
      ],
      "metadata": {
        "id": "ozDFHD4w206j"
      },
      "id": "ozDFHD4w206j"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYYL_jnG0B3i"
      },
      "source": [
        "# <font color = 'indianred'>**Regression using Pytroch**</font>\n",
        "\n",
        "Imagine that you're trying to figure out relationship between two variables x and y . You have some idea but you aren't quite sure yet whether the dependence is linear or quadratic.\n",
        "\n",
        "Your goal is to use least mean squares regression to identify the coefficients for the following three models. The three models are:\n",
        "\n",
        "1. Quadratic model where $\\mathrm{y} = b + w_1 \\cdot \\mathrm{x} + w_2 \\cdot \\mathrm{x}^2$.\n",
        "1. Linear model where $\\mathrm{y} = b + w_1 \\cdot \\mathrm{x}$.\n",
        "1. Linear model with no bias  where $\\mathrm{y} = w_1 \\cdot \\mathrm{x}$.\n",
        "\n",
        "- You will use <font color = 'indianred'>**Batch gradient descent to estimate the model co-efficients.Batch gradient descent uses complete training data at each iteration.**</font>\n",
        "- You will implement only training loop (no splitting of data in to training/validation).\n",
        "- The training loop will have only one ```for loop```. We need to iterate over whole data in each epoch. We do not need to create batches.\n",
        "- You may have to try different values of number of epochs/ learning rate to get good results.\n",
        "- You should use  Pytorch's nn.module and functions."
      ],
      "id": "BYYL_jnG0B3i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JShaYAruM_9F"
      },
      "source": [
        "## <font color = 'indianred'> **Data**"
      ],
      "id": "JShaYAruM_9F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUcQ0mpp3HNm"
      },
      "outputs": [],
      "source": [
        "x = torch.tensor([1.5420291, 1.8935232, 2.1603365, 2.5381863, 2.893443, \\\n",
        "                    3.838855, 3.925425, 4.2233696, 4.235571, 4.273397, \\\n",
        "                    4.9332876, 6.4704757, 6.517571, 6.87826, 7.0009003, \\\n",
        "                    7.035741, 7.278681, 7.7561755, 9.121138, 9.728281])\n",
        "y = torch.tensor([63.802246, 80.036026, 91.4903, 108.28776, 122.781975, \\\n",
        "                    161.36314, 166.50816, 176.16772, 180.29395, 179.09758, \\\n",
        "                    206.21027, 272.71857, 272.24033, 289.54745, 293.8488, \\\n",
        "                    295.2281, 306.62274, 327.93243, 383.16296, 408.65967])"
      ],
      "id": "WUcQ0mpp3HNm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rANPOz2gM6jM"
      },
      "outputs": [],
      "source": [
        "# Reshapeed the y tensor to have shape (n, 1), where n is the number of samples.\n",
        "# This is done to match the expected input shape for PyTorch's loss functions.\n",
        "y = y.view(-1,1)\n",
        "\n",
        "# Reshapeed the x tensor to have shape (n, 1), similar to y, for consistency and to work with matrix operations.\n",
        "x = x.view(-1,1)\n",
        "\n",
        "# Computed the square of each element in x.\n",
        "# This may be used for polynomial features in regression models.\n",
        "x2 = x * x\n"
      ],
      "id": "rANPOz2gM6jM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqQS3ENqNE2l"
      },
      "outputs": [],
      "source": [
        "# Concatenated the original x tensor and its squared values (x2) along dimension 1 (columns).\n",
        "# This creates a new tensor with two features: the original x and x2 (its square) . This can be useful for polynomial regression.\n",
        "x_combined = torch.cat((x, x2), dim= 1)\n"
      ],
      "id": "hqQS3ENqNE2l"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S50TcA7JNNtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d81cbe-6424-4f22-99d9-3ce588e8ef26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([20, 2]) torch.Size([20, 1])\n"
          ]
        }
      ],
      "source": [
        "print(x_combined.shape, x.shape)"
      ],
      "id": "S50TcA7JNNtK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcxV6FYJo_Z6"
      },
      "source": [
        "##<font color = 'indianred'>**Loss Function**"
      ],
      "id": "mcxV6FYJo_Z6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT8GelQSpFSR"
      },
      "outputs": [],
      "source": [
        "# Initialized Mean Squared Error (MSE) loss function with mean reduction\n",
        "# 'reduction=\"mean\"' averages the squared differences between predicted and target values\n",
        "loss_function = nn.MSELoss(reduction='mean')\n"
      ],
      "id": "HT8GelQSpFSR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbT4Z89HHvPb"
      },
      "source": [
        "## <font color = 'indianred'> **Train Function**"
      ],
      "id": "BbT4Z89HHvPb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSJU5UgpNy6b"
      },
      "outputs": [],
      "source": [
        "def train(epochs, x, y, loss_function, log_interval, model, optimizer):\n",
        "    \"\"\"\n",
        "    Train a PyTorch model using gradient descent.\n",
        "\n",
        "    Parameters:\n",
        "    epochs (int): The number of training epochs.\n",
        "    x (torch.Tensor): The input features.\n",
        "    y (torch.Tensor): The ground truth labels.\n",
        "    loss_function (torch.nn.Module): The loss function to be minimized.\n",
        "    log_interval (int): The interval at which training information is logged.\n",
        "    model (torch.nn.Module): The PyTorch model to be trained.\n",
        "    optimizer (torch.optim.Optimizer): The optimizer for updating model parameters.\n",
        "\n",
        "    Side Effects:\n",
        "    - Modifies the input model's internal parameters during training.\n",
        "    - Outputs training log information at specified intervals.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        optim = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
        "\n",
        "        # Step 1: Forward pass - Compute predictions based on the input features\n",
        "        y_hat = model(x)\n",
        "\n",
        "        # Step 2: Compute Loss\n",
        "        loss = loss_function(y_hat,y)\n",
        "\n",
        "        # Step 3: Zero Gradients - Clear previous gradient information to prevent accumulation\n",
        "        optim.zero_grad()\n",
        "\n",
        "        # Step 4: Calculate Gradients - Backpropagate the error to compute gradients for each parameter\n",
        "        loss.backward()\n",
        "\n",
        "        # Step 5: Update Model Parameters - Adjust weights based on computed gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        # Log training information at specified intervals\n",
        "        if epoch % log_interval == 0:\n",
        "            print(f'epoch: {epoch + 1} --> loss {loss.item()}')\n",
        "\n"
      ],
      "id": "FSJU5UgpNy6b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xns2sBYJm0dj"
      },
      "source": [
        "## <font color = 'indianred'> **Part 1**\n",
        "\n",
        "-  <font color = 'indianred'>**For Part 1, use x_combined (we need to use both $x$ and $x^2 $) as input to the model, this means that you have two inputs.**</font>\n",
        "- Use `nn.Linear` function to specify the model, <font color = 'indianred'>**think carefully what values the three arguments ```(n_ins, n_outs, bias)``` will take**.</font>.\n",
        "- In PyTorch, the `nn.Linear` layer initializes its weights using Kaiming (He) initialization by default, which is well-suited for ReLU activation functions. The bias terms are initialized to zero.\n",
        "-  In this task we will  use `nn.init` functions like `nn.init.normal_` and `nn.init.zeros_`, to explicitly override these default initializations to use your specified methods.\n",
        "\n",
        "\n",
        "**Run the cell below twice**\n",
        "\n",
        "**In the first attempt**\n",
        "- Use LEARNING_RATE = 0.05\n",
        "What do we observe?\n",
        "\n",
        "Write your observations HERE:\n",
        "\n",
        "**In the second attempt**\n",
        "- Now use a LEARNING_RATE  = 0.0005,\n",
        "What do we observe?\n",
        "\n",
        "Write your observations HERE:\n"
      ],
      "id": "xns2sBYJm0dj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utgwb7HyRZkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a74e5adf-3ea8-4b14-fb9b-4edc459d34a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 --> loss 57958.375\n",
            "epoch: 10001 --> loss 5.003321647644043\n",
            "epoch: 20001 --> loss 3.095216989517212\n",
            "epoch: 30001 --> loss 2.1377272605895996\n",
            "epoch: 40001 --> loss 1.657240867614746\n",
            "epoch: 50001 --> loss 1.4161328077316284\n",
            "epoch: 60001 --> loss 1.2949767112731934\n",
            "epoch: 70001 --> loss 1.2341285943984985\n",
            "epoch: 80001 --> loss 1.2036001682281494\n",
            "epoch: 90001 --> loss 1.1881986856460571\n"
          ]
        }
      ],
      "source": [
        "# model 1\n",
        "LEARNING_RATE = 0.0005\n",
        "EPOCHS = 100000\n",
        "LOG_INTERVAL= 10000\n",
        "\n",
        "# Use PyTorch's nn.Linear to create the model for your task.\n",
        "# Based on your understanding of the problem at hand, decide how you will initialize the nn.Linear layer.\n",
        "# Take into consideration the number of input features, the number of output features, and whether or not to include a bias term.\n",
        "model = nn.Linear(2,1)\n",
        "\n",
        "# Initialize the weights of the model using a normal distribution with mean = 0 and std = 0.01\n",
        "# Hint: To initialize the model's weights, you can use the nn.init.normal_() function.\n",
        "# You will need to provide the 'model.weight' tensor and specify values for the 'mean' and 'std' arguments.\n",
        "nn.init.normal_(model.weight,mean=0,std=0.001)\n",
        "\n",
        "\n",
        "# Initialize the model's bias terms to zero\n",
        "# Hint: To set the model's bias terms to zero, consider using the nn.init.zeros_() function.\n",
        "# You'll need to supply 'model.bias' as an argument.\n",
        "nn.init.zeros_(model.bias)\n",
        "\n",
        "# Create an SGD (Stochastic Gradient Descent) optimizer using the model's parameters and a predefined learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# Start the training process for the model with specified parameters and settings\n",
        "train(EPOCHS, x_combined, y, loss_function, LOG_INTERVAL, model, optimizer)\n"
      ],
      "id": "utgwb7HyRZkm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYu7XpVUXe6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ded6118c-91f7-4750-db0c-db9b13d3bfc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Weights tensor([[4.1796e+01, 1.4826e-02]]), \n",
            "Bias: tensor([0.9774])\n"
          ]
        }
      ],
      "source": [
        "print(f' Weights {model.weight.data}, \\nBias: {model.bias.data}')"
      ],
      "id": "vYu7XpVUXe6F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjBjky9Hm7Up"
      },
      "source": [
        "## <font color = 'indianred'> **Part 2**\n",
        "\n",
        "-  <font color = 'indianred'>**For Part 2, used $x$ as input to the model, this means that you have only one input.**</font>\n",
        "- Use `nn.Linear` to specify the model, <font color = 'indianred'>**think carefully what values the three arguments ```(n_ins, n_outs, bias)``` will take**.</font>.\n"
      ],
      "id": "yjBjky9Hm7Up"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELy7vX8Fm7Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c565c3-bff9-430b-cd78-4da9551aa0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 --> loss 57976.9453125\n",
            "epoch: 11 --> loss 6.975368499755859\n",
            "epoch: 21 --> loss 6.601193428039551\n",
            "epoch: 31 --> loss 6.251128196716309\n",
            "epoch: 41 --> loss 5.923625469207764\n",
            "epoch: 51 --> loss 5.617262840270996\n",
            "epoch: 61 --> loss 5.330683708190918\n",
            "epoch: 71 --> loss 5.062532901763916\n",
            "epoch: 81 --> loss 4.811709403991699\n",
            "epoch: 91 --> loss 4.577028751373291\n",
            "epoch: 101 --> loss 4.35750675201416\n",
            "epoch: 111 --> loss 4.152127265930176\n",
            "epoch: 121 --> loss 3.9599738121032715\n",
            "epoch: 131 --> loss 3.780254364013672\n",
            "epoch: 141 --> loss 3.6120963096618652\n",
            "epoch: 151 --> loss 3.4547812938690186\n",
            "epoch: 161 --> loss 3.3076419830322266\n",
            "epoch: 171 --> loss 3.1699490547180176\n",
            "epoch: 181 --> loss 3.041151523590088\n",
            "epoch: 191 --> loss 2.9206676483154297\n",
            "epoch: 201 --> loss 2.807941436767578\n",
            "epoch: 211 --> loss 2.7024998664855957\n",
            "epoch: 221 --> loss 2.603853940963745\n",
            "epoch: 231 --> loss 2.511547327041626\n",
            "epoch: 241 --> loss 2.4252114295959473\n",
            "epoch: 251 --> loss 2.3444480895996094\n",
            "epoch: 261 --> loss 2.268883228302002\n",
            "epoch: 271 --> loss 2.198186159133911\n",
            "epoch: 281 --> loss 2.132059335708618\n",
            "epoch: 291 --> loss 2.070189952850342\n",
            "epoch: 301 --> loss 2.012310028076172\n",
            "epoch: 311 --> loss 1.9581661224365234\n",
            "epoch: 321 --> loss 1.9075130224227905\n",
            "epoch: 331 --> loss 1.8601264953613281\n",
            "epoch: 341 --> loss 1.8157867193222046\n",
            "epoch: 351 --> loss 1.774317979812622\n",
            "epoch: 361 --> loss 1.7355130910873413\n",
            "epoch: 371 --> loss 1.6992155313491821\n",
            "epoch: 381 --> loss 1.6652616262435913\n",
            "epoch: 391 --> loss 1.633490800857544\n",
            "epoch: 401 --> loss 1.603784203529358\n",
            "epoch: 411 --> loss 1.575974702835083\n",
            "epoch: 421 --> loss 1.5499725341796875\n",
            "epoch: 431 --> loss 1.5256268978118896\n",
            "epoch: 441 --> loss 1.502870798110962\n",
            "epoch: 451 --> loss 1.4815771579742432\n",
            "epoch: 461 --> loss 1.4616546630859375\n",
            "epoch: 471 --> loss 1.4430243968963623\n",
            "epoch: 481 --> loss 1.4255808591842651\n",
            "epoch: 491 --> loss 1.409273386001587\n",
            "epoch: 501 --> loss 1.394014835357666\n",
            "epoch: 511 --> loss 1.3797348737716675\n",
            "epoch: 521 --> loss 1.3663774728775024\n",
            "epoch: 531 --> loss 1.3538867235183716\n",
            "epoch: 541 --> loss 1.342193841934204\n",
            "epoch: 551 --> loss 1.3312662839889526\n",
            "epoch: 561 --> loss 1.3210341930389404\n",
            "epoch: 571 --> loss 1.3114607334136963\n",
            "epoch: 581 --> loss 1.302513837814331\n",
            "epoch: 591 --> loss 1.2941372394561768\n",
            "epoch: 601 --> loss 1.2863026857376099\n",
            "epoch: 611 --> loss 1.2789688110351562\n",
            "epoch: 621 --> loss 1.2721104621887207\n",
            "epoch: 631 --> loss 1.2656954526901245\n",
            "epoch: 641 --> loss 1.2596943378448486\n",
            "epoch: 651 --> loss 1.2540829181671143\n",
            "epoch: 661 --> loss 1.2488380670547485\n",
            "epoch: 671 --> loss 1.2439173460006714\n",
            "epoch: 681 --> loss 1.2393168210983276\n",
            "epoch: 691 --> loss 1.2350198030471802\n",
            "epoch: 701 --> loss 1.231001377105713\n",
            "epoch: 711 --> loss 1.2272391319274902\n",
            "epoch: 721 --> loss 1.2237110137939453\n",
            "epoch: 731 --> loss 1.2204177379608154\n",
            "epoch: 741 --> loss 1.2173349857330322\n",
            "epoch: 751 --> loss 1.2144545316696167\n",
            "epoch: 761 --> loss 1.211758017539978\n",
            "epoch: 771 --> loss 1.2092320919036865\n",
            "epoch: 781 --> loss 1.206878900527954\n",
            "epoch: 791 --> loss 1.2046655416488647\n",
            "epoch: 801 --> loss 1.2026097774505615\n",
            "epoch: 811 --> loss 1.2006685733795166\n",
            "epoch: 821 --> loss 1.1988648176193237\n",
            "epoch: 831 --> loss 1.19717276096344\n",
            "epoch: 841 --> loss 1.1955827474594116\n",
            "epoch: 851 --> loss 1.1941139698028564\n",
            "epoch: 861 --> loss 1.1927242279052734\n",
            "epoch: 871 --> loss 1.1914260387420654\n",
            "epoch: 881 --> loss 1.1902191638946533\n",
            "epoch: 891 --> loss 1.189077377319336\n",
            "epoch: 901 --> loss 1.1880165338516235\n",
            "epoch: 911 --> loss 1.1870301961898804\n",
            "epoch: 921 --> loss 1.186102032661438\n",
            "epoch: 931 --> loss 1.1852357387542725\n",
            "epoch: 941 --> loss 1.1844183206558228\n",
            "epoch: 951 --> loss 1.1836566925048828\n",
            "epoch: 961 --> loss 1.1829482316970825\n",
            "epoch: 971 --> loss 1.1822822093963623\n",
            "epoch: 981 --> loss 1.1816635131835938\n",
            "epoch: 991 --> loss 1.1810787916183472\n"
          ]
        }
      ],
      "source": [
        "# model 2\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 1000\n",
        "LOG_INTERVAL= 10\n",
        "\n",
        "# Used PyTorch's nn.Linear to create the model for your task.\n",
        "# Based on your understanding of the problem at hand, decide how you will initialize the nn.Linear layer.\n",
        "# Taken into consideration the number of input features, the number of output features, and whether or not to include a bias term.\n",
        "model = nn.Linear(1,1,bias=True)\n",
        "\n",
        "# Initializde the weights of the model using a normal distribution with mean = 0 and std = 0.01\n",
        "# Hint: To initialize the model's weights, you can use the torch.nn.init.normal_() function.\n",
        "# We will need to provide the 'model.weight' tensor and specify values for the 'mean' and 'std' arguments.\n",
        "nn.init.normal_(model.weight,mean=0,std=0.01)\n",
        "\n",
        "\n",
        "\n",
        "# Initialized the model's bias terms to zero\n",
        "# Hint: To set the model's bias terms to zero, consider using the nn.init.zeros_() function.\n",
        "# We'll need to supply 'model.bias' as an argument.\n",
        "nn.init.zeros_(model.bias)\n",
        "\n",
        "\n",
        "# Created an SGD (Stochastic Gradient Descent) optimizer using the model's parameters and a predefined learning rate\n",
        "optimizer =  torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# Started the training process for the model with specified parameters and settings\n",
        "# Noted that we are passing x as an input for this part\n",
        "train(EPOCHS, x, y, loss_function, LOG_INTERVAL, model, optimizer)"
      ],
      "id": "ELy7vX8Fm7Up"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrd64AY7m7Up",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01edcf39-0555-4499-ca0c-a2a068e1c553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Weights tensor([[41.9377]]), \n",
            "Bias: tensor([0.7468])\n"
          ]
        }
      ],
      "source": [
        "print(f' Weights {model.weight.data}, \\nBias: {model.bias.data}')"
      ],
      "id": "hrd64AY7m7Up"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wE91_Rg8nmSu"
      },
      "source": [
        "## <font color = 'indianred'> **Part 3**\n",
        "-  <font color = 'indianred'>**Part 3 is similar to part 2, the only difference is that model has no bias term now.**</font>\n",
        "- **we will see that we are now running the model for only ten epochs and will get similar results**"
      ],
      "id": "wE91_Rg8nmSu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3QSWacbnmSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca95ff20-0a6d-42b9-bce4-2c19959ec020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 --> loss 57964.03125\n",
            "epoch: 2 --> loss 6895.87646484375\n",
            "epoch: 3 --> loss 821.3375244140625\n",
            "epoch: 4 --> loss 98.77323150634766\n",
            "epoch: 5 --> loss 12.824605941772461\n",
            "epoch: 6 --> loss 2.6011123657226562\n",
            "epoch: 7 --> loss 1.3850191831588745\n",
            "epoch: 8 --> loss 1.2403713464736938\n",
            "epoch: 9 --> loss 1.223166584968567\n",
            "epoch: 10 --> loss 1.2211220264434814\n"
          ]
        }
      ],
      "source": [
        "# model 3\n",
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 10\n",
        "LOG_INTERVAL= 1\n",
        "\n",
        "# Used PyTorch's nn.Linear to create the model for your task.\n",
        "# Based on your understanding of the problem at hand, decide how you will initialize the nn.Linear layer.\n",
        "# Taken into consideration the number of input features, the number of output features, and whether or not to include a bias term.\n",
        "model = nn.Linear(1,1,bias=False)\n",
        "\n",
        "\n",
        "# Initialized the weights of the model using a normal distribution with mean = 0 and std = 0.01\n",
        "# Hint used: To initialize the model's weights, you can use the nn.init.normal_() function.\n",
        "# we will need to provide the 'model.weight' tensor and specify values for the 'mean' and 'std' arguments.\n",
        "nn.init.normal_(model.weight,mean=0,std=0.01)\n",
        "\n",
        "\n",
        "\n",
        "# We do not need to initilaize the bias term as there is no bias term in this model\n",
        "\n",
        "# Create an SGD (Stochastic Gradient Descent) optimizer using the model's parameters and a predefined learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)\n",
        "\n",
        "\n",
        "# Start the training process for the model with specified parameters and settings\n",
        "# Note that we are passing x as an input for this part\n",
        "train(EPOCHS, x, y, loss_function, LOG_INTERVAL, model, optimizer)\n"
      ],
      "id": "-3QSWacbnmSu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7a2F_GZnmSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cc20d43-9d81-4a27-eb28-01b3b573aee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Weights tensor([[42.0557]])\n"
          ]
        }
      ],
      "source": [
        "print(f' Weights {model.weight.data}')"
      ],
      "id": "s7a2F_GZnmSu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'indianred'>**Task 3 - MultiClass Classification using Mini Batch Gradient Descent with PyTorch**\n",
        "\n",
        "- You will implement only training loop (no splitting of data in to training/validation).\n",
        "- We will use minibatch Gradient Descent - Hence we will have two for loops in his case.\n",
        "- You should use  Pytorch's nn.module and functions."
      ],
      "metadata": {
        "id": "wxiVbbqL8GIa"
      },
      "id": "wxiVbbqL8GIa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6jh4FfFPgi_"
      },
      "source": [
        "## <font color = 'indianred'>**Data**"
      ],
      "id": "o6jh4FfFPgi_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-26T19:14:42.405079Z",
          "iopub.status.busy": "2022-10-26T19:14:42.404795Z",
          "iopub.status.idle": "2022-10-26T19:14:42.725729Z",
          "shell.execute_reply": "2022-10-26T19:14:42.725116Z",
          "shell.execute_reply.started": "2022-10-26T19:14:42.405000Z"
        },
        "id": "1pN2psBZPquN"
      },
      "outputs": [],
      "source": [
        "# Imported the make_classification function from the sklearn.datasets module\n",
        "# This function is used to generate a synthetic dataset for classification tasks.\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "# Imported the StandardScaler class from the sklearn.preprocessing module\n",
        "# StandardScaler is used to standardize the features by removing the mean and scaling to unit variance.\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "id": "1pN2psBZPquN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHjBoauBQ809"
      },
      "outputs": [],
      "source": [
        "# Imported the main PyTorch library, which provides the essential building blocks for constructing neural networks.\n",
        "import torch\n",
        "\n",
        "# Imported the 'optim' module from PyTorch for various optimization algorithms like SGD, Adam, etc.\n",
        "import torch.optim as optim\n",
        "\n",
        "# Imported the 'nn' module from PyTorch, which contains pre-defined layers, loss functions, etc., for neural networks.\n",
        "import torch.nn as nn\n",
        "\n",
        "# Imported the 'functional' module from PyTorch; incorrect import here, it should be 'import torch.nn.functional as F'\n",
        "# This module contains functional forms of layers, loss functions, and other operations.\n",
        "import torch.functional as F  # Should be 'import torch.nn.functional as F'\n",
        "\n",
        "# Imported DataLoader and Dataset classes from PyTorch's utility library.\n",
        "# DataLoader helps with batching, shuffling, and loading data in parallel.\n",
        "# Dataset provides an abstract interface for easier data manipulation.\n",
        "from torch.utils.data import DataLoader, Dataset\n"
      ],
      "id": "dHjBoauBQ809"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BClv_rE78pCd"
      },
      "outputs": [],
      "source": [
        "# Generated a synthetic dataset for classification using make_classification function.\n",
        "# Parameters:\n",
        "# - n_samples=1000: The total number of samples in the generated dataset.\n",
        "# - n_features=5: The total number of features for each sample.\n",
        "# - n_classes=3: The number of classes for the classification task.\n",
        "# - n_informative=4: The number of informative features, i.e., features that are actually useful for classification.\n",
        "# - n_redundant=1: The number of redundant features, i.e., features that can be linearly derived from informative features.\n",
        "# - random_state=0: The seed for the random number generator to ensure reproducibility.\n",
        "\n",
        "X, y = make_classification(n_samples=1000, n_features=5, n_classes=3, n_informative=4, n_redundant=1, random_state=0)\n"
      ],
      "id": "BClv_rE78pCd"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this example, we're using `make_classification` to <font color = 'indianred'>**generates a dataset with 1,000 samples, 5 features per sample, and 3 classes for the classification problem**.</font> Of the 5 features, 4 are informative (useful for classification), and 1 is redundant (can be derived from the informative features). The `random_state` parameter ensures that the data generation is reproducible."
      ],
      "metadata": {
        "id": "SxLvScaHKueM"
      },
      "id": "SxLvScaHKueM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl3r5LUd8pCe"
      },
      "outputs": [],
      "source": [
        "# Initialized the StandardScaler object from the sklearn.preprocessing module.\n",
        "# This will be used to standardize the features of the dataset.\n",
        "preprocessor = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler on the dataset (X) and then transform it.\n",
        "# The fit_transform() method computes the mean and standard deviation of each feature,\n",
        "# and then standardizes the features by subtracting the mean and dividing by the standard deviation.\n",
        "X = preprocessor.fit_transform(X)\n"
      ],
      "id": "pl3r5LUd8pCe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKQM7PcKVDkK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80833c3e-7b18-4cfb-b3a7-c4f10dcc1d9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 5) (1000,)\n"
          ]
        }
      ],
      "source": [
        "print(X.shape, y.shape)"
      ],
      "id": "IKQM7PcKVDkK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhLylG90VHG5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fea11b22-49c1-4b9c-8cbe-7b1ae092fc89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.39443436, -0.78033571, -0.25005511,  0.09118536, -0.5690698 ],\n",
              "       [ 0.64284479, -0.95837057,  0.83598996, -0.08438568,  0.50539358],\n",
              "       [ 0.99102498,  0.8580679 ,  0.78786062, -0.9114329 ,  1.62615938],\n",
              "       [-0.96923966,  0.86168226, -1.31837608, -1.22844863, -0.07591589],\n",
              "       [ 0.96021518,  0.99206623,  1.0026402 , -0.25339161,  1.18831784]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "X[0:5]"
      ],
      "id": "YhLylG90VHG5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8ZlJ168VLQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ba83ad-2277-4310-8b76-27f577bad06a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2 0 1 2 1 1 0 2 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(y[0:10])"
      ],
      "id": "a8ZlJ168VLQL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUYUlTaEQW2w"
      },
      "source": [
        "## <font color = 'indianred'>**Dataset and Data Loaders**"
      ],
      "id": "tUYUlTaEQW2w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3kFQdLC8pCf"
      },
      "outputs": [],
      "source": [
        "# Convert the numpy arrays X and y to PyTorch Tensors.\n",
        "# For X, we create a floating-point tensor since most PyTorch models expect float inputs for features.\n",
        "# This is a  multiclass classification problem.\n",
        "\n",
        "# ================================\n",
        "# IMPORTANT: # Consider what cost function you will use and whether it expects the label tensor (y)  to be float or long type.\n",
        "# ================================\n",
        "\n",
        "x_tensor = torch.tensor(X)\n",
        "y_tensor = torch.tensor(y)\n"
      ],
      "id": "x3kFQdLC8pCf"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom PyTorch Dataset class for handling our data\n",
        "class MyDataset(Dataset):\n",
        "    # Constructor: Initialize the dataset with features and labels\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    # Method to return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    # Method to get a data point by index\n",
        "    def __getitem__(self, index):\n",
        "        x = self.features[index]\n",
        "        y = self.labels[index]\n",
        "        return x, y\n",
        "\n"
      ],
      "metadata": {
        "id": "IqZiLscT-LvT"
      },
      "id": "IqZiLscT-LvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the custom MyDataset class, passing in the feature and label tensors.\n",
        "# This will allow the data to be used with PyTorch's DataLoader for efficient batch processing.\n",
        "train_dataset = MyDataset(x_tensor, y_tensor)\n"
      ],
      "metadata": {
        "id": "70FnT-L6-P96"
      },
      "id": "70FnT-L6-P96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first element (feature-label pair) from the train_dataset using indexing.\n",
        "# The __getitem__ method of MyDataset class will be called to return this element.\n",
        "train_dataset[0]\n"
      ],
      "metadata": {
        "id": "KwjHPEoZ-Sfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19db93e7-3470-47b8-b742-605eeafbf5bb"
      },
      "id": "KwjHPEoZ-Sfb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.3944, -0.7803, -0.2501,  0.0912, -0.5691], dtype=torch.float64),\n",
              " tensor(2))"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Data loader from Dataset\n",
        "# Use a batch size of 16\n",
        "# Use shuffle = True\n",
        "train_loader = DataLoader(train_dataset,batch_size = 16,shuffle=True)"
      ],
      "metadata": {
        "id": "lxMxiAKn-FP8"
      },
      "id": "lxMxiAKn-FP8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3HFDKy1RLMj"
      },
      "source": [
        "## <font color = 'indianred'>**Model**"
      ],
      "id": "y3HFDKy1RLMj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtFhXfXpROP3"
      },
      "outputs": [],
      "source": [
        "# Task: Define your neural network model for multi-class classification.\n",
        "# Thought through what layers you should add. Note: My task is to create a model that uses Softmax for\n",
        "# classification but doesn't include any hidden layers.\n",
        "# We can use nn.Linear or nn.Sequential for this task\n",
        "model = nn.Linear(5,3,bias=True)\n",
        "\n"
      ],
      "id": "mtFhXfXpROP3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'indianred'>**Loss Function**"
      ],
      "metadata": {
        "id": "bDkkl8M2c7xo"
      },
      "id": "bDkkl8M2c7xo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyxG9cEIRdxD"
      },
      "outputs": [],
      "source": [
        "# Task: Specify the loss function for your model.\n",
        "# Considered the architecture of my model, especially the last layer, when choosing the loss function.\n",
        "# Reminder: The last layer in the previous step should guide your choice for an appropriate loss function for multi-class classification.\n",
        "\n",
        "loss_function =  nn.CrossEntropyLoss()"
      ],
      "id": "ZyxG9cEIRdxD"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0ETm1C0apXS"
      },
      "source": [
        "## <font color = 'indianred'>**Initialization**\n",
        "\n",
        "Created a function to initilaize weights.\n",
        "- Initialized weights using normal distribution with mean = 0 and std = 0.05\n",
        "- Initilaized the bias term with zeros"
      ],
      "id": "Z0ETm1C0apXS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7_uk-Wya5w-"
      },
      "outputs": [],
      "source": [
        "# Function to initialize the weights and biases of a neural network layer.\n",
        "# This function specifically targets layers of type nn.Linear.\n",
        "def init_weights(layer):\n",
        "  # Check if the layer is of the type nn.Linear.\n",
        "  if type(layer) == nn.Linear:\n",
        "    # Initialize the weights with a normal distribution, centered at 0 with a standard deviation of 0.05.\n",
        "    torch.nn.init.normal_(layer.weight, mean=0, std=0.05)\n",
        "    # Initialize the bias terms to zero.\n",
        "    torch.nn.init.zeros_(layer.bias)\n"
      ],
      "id": "P7_uk-Wya5w-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s37DSbzSkWF"
      },
      "source": [
        "## <font color = 'indianred'>**Training Loop**"
      ],
      "id": "_s37DSbzSkWF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m8CNyv7Syxx"
      },
      "source": [
        "**Model Training** involves five steps:\n",
        "\n",
        "- Step 0: Randomly initialize parameters / weights\n",
        "- Step 1: Compute model's predictions - forward pass\n",
        "- Step 2: Compute loss\n",
        "- Step 3: Compute the gradients\n",
        "- Step 4: Update the parameters\n",
        "- Step 5: Repeat steps 1 - 4\n",
        "\n",
        "Model training is repeating this process over and over, for many **epochs**.\n",
        "\n",
        "We will specify number of ***epochs*** and during each epoch we will iterate over the complete dataset and will keep on updating the parameters.\n",
        "\n",
        "***Learning rate*** and ***epochs*** are known as hyperparameters. We have to adjust the values of these two based on validation dataset.\n",
        "\n",
        "We will now create functions for step 1 to 4."
      ],
      "id": "0m8CNyv7Syxx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRFZkBl-TGPC"
      },
      "outputs": [],
      "source": [
        "# Function to train a neural network model.\n",
        "# Arguments include the number of epochs, loss function, learning rate, model architecture, and optimizer.\n",
        "\n",
        "def train(epochs, loss_function, learning_rate, model, optimizer):\n",
        "\n",
        "  # Loop through each epoch\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # Initialized variables to hold aggregated training loss and correct prediction count for each epoch\n",
        "    running_train_loss = 0\n",
        "    running_train_correct = 0\n",
        "\n",
        "    # Loop through each batch in the training dataset using train_loader\n",
        "    for x, y in train_loader:\n",
        "\n",
        "      # Move input and target tensors to the device (GPU or CPU)\n",
        "      device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "      x = x.to(device,dtype= torch.float32)\n",
        "      targets = y.to(device,dtype=torch.long)\n",
        "\n",
        "\n",
        "      # Step 1: Forward Pass: Compute model's predictions\n",
        "      output = model(x)\n",
        "\n",
        "      # Step 2: Compute loss\n",
        "      loss = loss_function(output,targets)\n",
        "\n",
        "      # Step 3: Backward pass - Compute the gradients\n",
        "      # Zero out gradients from the previous iteration\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Backward pass: Compute gradients based on the loss\n",
        "      loss.backward()\n",
        "\n",
        "      # Step 4: Update the parameters\n",
        "      optimizer.step()\n",
        "\n",
        "      # Accumulated the loss for the batch\n",
        "      running_train_loss += loss.item()\n",
        "\n",
        "      # Evaluated model's performance without backpropagation for efficiency\n",
        "      # `with torch.no_grad()` temporarily disables autograd, improving speed and avoiding side effects during evaluation.\n",
        "      with torch.no_grad():\n",
        "          y_pred =  output.argmax(dim=1)  # Find the class index with the maximum predicted probability\n",
        "          correct = (y_pred==targets).sum().item() # Compute the number of correct predictions in the batch\n",
        "          running_train_correct += correct  # Update the cumulative count of correct predictions for the current epoch\n",
        "\n",
        "\n",
        "    # Computed average training loss and accuracy for the epoch\n",
        "    train_loss = running_train_loss / len(train_loader)\n",
        "    train_acc = running_train_correct / len(train_loader.dataset)\n",
        "\n",
        "    # Displayed training loss and accuracy metrics for the current epoch\n",
        "    print(f'Epoch : {epoch + 1} / {epochs}')\n",
        "    print(f'Train Loss: {train_loss:.4f} | Train Accuracy: {train_acc * 100:.4f}%')\n",
        "\n"
      ],
      "id": "zRFZkBl-TGPC"
    },
    {
      "cell_type": "code",
      "source": [
        "# Fixed the random seed to ensure reproducibility across runs\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# Defined the total number of epochs for which the model will be trained\n",
        "epochs = 5\n",
        "\n",
        "# Detected if a GPU is available and use it; otherwise, use CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)  # Output the device being used\n",
        "\n",
        "# Defined the learning rate for optimization; consider its impact on model performance\n",
        "learning_rate = 1\n",
        "\n",
        "# Task: Configure the optimizer for model training.\n",
        "# Here, we're using Stochastic Gradient Descent (SGD). Think through what parameters are needed.\n",
        "# Reminder: Utilize the learning rate defined above when setting up your optimizer.\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "\n",
        "# Relocate the model to the appropriate compute device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Apply custom weight initialization; this can affect the model's learning trajectory\n",
        "# The `apply` function recursively applies a function to each submodule in a PyTorch model.\n",
        "# In the given context, it's used to apply the `init_weights` function to initialize the weights of all layers in the model.\n",
        "# The benefit is that it provides a convenient way to systematically apply custom weight initialization across complex models,\n",
        "# potentially improving model convergence and performance.\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Kick off the training process using the specified settings\n",
        "train(epochs, loss_function, learning_rate, model, optimizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "gkb82Lb6DQXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5590b15d-6693-4694-b766-1146a4101038"
      },
      "id": "gkb82Lb6DQXp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "Epoch : 1 / 5\n",
            "Train Loss: 0.8130 | Train Accuracy: 65.4000%\n",
            "Epoch : 2 / 5\n",
            "Train Loss: 0.8005 | Train Accuracy: 66.7000%\n",
            "Epoch : 3 / 5\n",
            "Train Loss: 0.7915 | Train Accuracy: 67.0000%\n",
            "Epoch : 4 / 5\n",
            "Train Loss: 0.7978 | Train Accuracy: 67.9000%\n",
            "Epoch : 5 / 5\n",
            "Train Loss: 0.8008 | Train Accuracy: 67.0000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V3o_7WCYWac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c6a3e25-9476-4500-90e2-edd57824e3c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight tensor([[ 0.4345, -0.9407, -0.5891, -0.4591,  0.7364],\n",
            "        [ 0.0557,  1.0332,  0.1920,  0.4732,  0.0554],\n",
            "        [-0.6367, -0.0987,  0.2159,  0.0453, -0.8418]])\n",
            "bias tensor([-0.2508, -0.0254,  0.2762])\n"
          ]
        }
      ],
      "source": [
        "# Output the learned parameters (weights and biases) of the model after training\n",
        "for name, param in model.named_parameters():\n",
        "  # Print the name and the values of each parameter\n",
        "  print(name, param.data)\n"
      ],
      "id": "6V3o_7WCYWac"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font color = 'indianred'>**Task 4 - MultiLabel Classification using Mini Batch Gradient Descent with PyTorch**\n",
        "\n",
        "- We will implement only training loop (no splitting of data in to training/validation).\n",
        "- We will use minibatch Gradient Descent - Hence we will have two for loops in his case.\n",
        "- We use  Pytorch's nn.module and functions."
      ],
      "metadata": {
        "id": "UArlfemmD0Wk"
      },
      "id": "UArlfemmD0Wk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Dypy43D0Wk"
      },
      "source": [
        "## <font color = 'indianred'>**Data**"
      ],
      "id": "G7Dypy43D0Wk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-10-26T19:14:42.405079Z",
          "iopub.status.busy": "2022-10-26T19:14:42.404795Z",
          "iopub.status.idle": "2022-10-26T19:14:42.725729Z",
          "shell.execute_reply": "2022-10-26T19:14:42.725116Z",
          "shell.execute_reply.started": "2022-10-26T19:14:42.405000Z"
        },
        "id": "iFG2Z-vXD0Wk"
      },
      "outputs": [],
      "source": [
        "# Imported the function to generate a synthetic multilabel classification dataset\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "\n",
        "# Imported the StandardScaler for feature normalization\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "id": "iFG2Z-vXD0Wk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BlPfRboD0Wl"
      },
      "outputs": [],
      "source": [
        "# Imported PyTorch library for tensor computation and neural network modules\n",
        "import torch\n",
        "\n",
        "# Imported PyTorch's optimization algorithms package\n",
        "import torch.optim as optim\n",
        "\n",
        "# Imported PyTorch's neural network module for defining layers and models\n",
        "import torch.nn as nn\n",
        "\n",
        "# Imported PyTorch's functional API for stateless operations\n",
        "import torch.functional as F\n",
        "\n",
        "# Imported DataLoader, TensorDataset, and Dataset for data loading and manipulation\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n"
      ],
      "id": "6BlPfRboD0Wl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2QhwfmoD0Wl"
      },
      "outputs": [],
      "source": [
        "# Generated a synthetic multilabel classification dataset\n",
        "# n_samples: Number of samples in the dataset\n",
        "# n_features: Number of feature variables\n",
        "# n_classes: Number of distinct labels (or classes)\n",
        "# n_labels: Average number of labels per instance\n",
        "# random_state: Seed for reproducibility\n",
        "X, y = make_multilabel_classification(n_samples=1000, n_features=5, n_classes=3, n_labels=2, random_state=0)\n"
      ],
      "id": "V2QhwfmoD0Wl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GMhD31WlD0Wl"
      },
      "outputs": [],
      "source": [
        "# Initialized the StandardScaler for feature normalization\n",
        "preprocessor = StandardScaler()\n",
        "\n",
        "# Fit the preprocessor to the data and transform the features for zero mean and unit variance\n",
        "X = preprocessor.fit_transform(X)\n"
      ],
      "id": "GMhD31WlD0Wl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TolNK76YD0Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2e4b06-58ec-4c50-e3be-af61a8641cc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 5) (1000, 3)\n"
          ]
        }
      ],
      "source": [
        "# Printed the shape of the feature matrix X and the label matrix y\n",
        "# Pay attention to these shapes as they will guide you in defining your neural network model\n",
        "print(X.shape, y.shape)\n"
      ],
      "id": "TolNK76YD0Wl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgD0GuF4D0Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20ff2ff9-0f80-40e6-9a20-e6fa669995c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.65506353,  0.2101857 ,  0.51570947, -2.00177184,  0.40001786],\n",
              "       [-0.02349989, -0.51376047,  2.34771468,  0.78787635, -1.04334554],\n",
              "       [ 1.09554239,  0.93413188, -0.09495894, -0.00916599, -0.01237169],\n",
              "       [-0.58302103,  1.17544727,  0.21037527, -0.80620833,  0.8124074 ],\n",
              "       [ 1.09554239,  0.69281649, -1.92696415,  1.18639752, -1.24954031]])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "X[0:5]"
      ],
      "id": "MgD0GuF4D0Wl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCki6N5XD0Wl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f0b7e8d-e859-42d1-d9f7-817075d41ee5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1]\n",
            " [1 0 0]\n",
            " [1 1 1]\n",
            " [0 1 1]\n",
            " [1 1 0]\n",
            " [0 1 0]\n",
            " [1 1 1]\n",
            " [1 0 1]\n",
            " [1 1 1]\n",
            " [1 1 0]]\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# IMPORTANT: # NOTE: The y in this case is one hot encoded.\n",
        "# This is different from Multiclass Classification.\n",
        "# The loss function we use for multiclass classification handles this internally\n",
        "# For multilabel case we have to provide y in this format\n",
        "# ================================\n",
        "\n",
        "print(y[0:10])"
      ],
      "id": "qCki6N5XD0Wl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuU4DdG4D0Wl"
      },
      "source": [
        "## <font color = 'indianred'>**Dataset and Data Loaders**"
      ],
      "id": "xuU4DdG4D0Wl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o-XzO8ID0Wm"
      },
      "outputs": [],
      "source": [
        "# Created Tensors from the numpy arrays.\n",
        "# Earlier, we focused on multiclass classification; now, we are dealing with multilabel classification.\n",
        "\n",
        "# ================================\n",
        "# IMPORTANT: # Consider what cost function you will use for multilabel classification and whether it expects the label tensor (y) to be float or long type.\n",
        "# ================================\n",
        "\n",
        "x_tensor = torch.tensor(X)\n",
        "y_tensor = torch.tensor(y)\n"
      ],
      "id": "3o-XzO8ID0Wm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined a custom PyTorch Dataset class for handling our data\n",
        "class MyDataset(Dataset):\n",
        "    # Constructor: Initialize the dataset with features and labels\n",
        "    def __init__(self, X, y):\n",
        "        self.features = X\n",
        "        self.labels = y\n",
        "\n",
        "    # Method to return the length of the dataset\n",
        "    def __len__(self):\n",
        "        return self.labels.shape[0]\n",
        "\n",
        "    # Method to get a data point by index\n",
        "    def __getitem__(self, index):\n",
        "        x = self.features[index]\n",
        "        y = self.labels[index]\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "YhIDfdgQD0Wm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "YhIDfdgQD0Wm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialized an instance of the custom MyDataset class\n",
        "# This will be our training dataset, holding our features and labels as PyTorch tensors\n",
        "train_dataset = MyDataset(x_tensor, y_tensor)\n"
      ],
      "metadata": {
        "id": "RSG9tzzND0Wm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "RSG9tzzND0Wm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessed the first element (feature-label pair) from the train_dataset using indexing.\n",
        "# The __getitem__ method of MyDataset class will be called to return this element.\n",
        "# This is useful for debugging and understanding the data structure\n",
        "train_dataset[0]\n"
      ],
      "metadata": {
        "id": "Vu2OtlMiD0Wm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f60d2a24-7afc-496e-b618-e376adfecf8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1.6551,  0.2102,  0.5157, -2.0018,  0.4000], dtype=torch.float64),\n",
              " tensor([0, 0, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "id": "Vu2OtlMiD0Wm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Created Data lOader from Dataset\n",
        "# Used a batch size of 16\n",
        "# Used shuffle = True\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=True)\n"
      ],
      "metadata": {
        "id": "m2wpBNxGD0Wm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "m2wpBNxGD0Wm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GHaXZjYD0Wm"
      },
      "source": [
        "## <font color = 'indianred'>**Model**"
      ],
      "id": "_GHaXZjYD0Wm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFO9egt-D0Wm"
      },
      "outputs": [],
      "source": [
        "# Task: Specify your model architecture here.\n",
        "# This is a multilabel problem. Think through what layers you should add to handle this.\n",
        "# Remember, the architecture of your last layer will also depend on your choice of loss function.\n",
        "# Additional Note: No hidden layers should be added for this exercise.\n",
        "# You can use nn.Linear or nn.Sequential for this task\n",
        "\n",
        "model = nn.Linear(5,3,bias=True)\n",
        "\n",
        "\n"
      ],
      "id": "hFO9egt-D0Wm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <font color = 'indianred'>**Loss Function**"
      ],
      "metadata": {
        "id": "kDAWf_4tD0Wm"
      },
      "id": "kDAWf_4tD0Wm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf_-zc8bD0Wn"
      },
      "outputs": [],
      "source": [
        "# Task: Specify the loss function for your model.\n",
        "# Considered the architecture of your model, especially the last layer, when choosing the loss function.\n",
        "# This is a multilabel problem, so make sure your choice reflects that.\n",
        "\n",
        "\n",
        "loss_function = torch.nn.BCEWithLogitsLoss()\n"
      ],
      "id": "Vf_-zc8bD0Wn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrg3Yjr2D0Wn"
      },
      "source": [
        "## <font color = 'indianred'>**Initialization**\n",
        "\n",
        "Created a function to initilaize weights.\n",
        "- Initialized weights using normal distribution with mean = 0 and std = 0.05\n",
        "- Initilaized the bias term with zeros"
      ],
      "id": "mrg3Yjr2D0Wn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-KxNiavD0Wn"
      },
      "outputs": [],
      "source": [
        "# Function to initialize the weights and biases of the model's layers\n",
        "# This is provided to you and is not a student task\n",
        "def init_weights(layer):\n",
        "  # Check if the layer is a Linear layer\n",
        "  if type(layer) == nn.Linear:\n",
        "    # Initialize the weights with a normal distribution, mean=0, std=0.05\n",
        "    torch.nn.init.normal_(layer.weight, mean = 0, std = 0.05)\n",
        "    # Initialize the bias terms to zero\n",
        "    torch.nn.init.zeros_(layer.bias)\n"
      ],
      "id": "a-KxNiavD0Wn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw1DGXWpD0Wn"
      },
      "source": [
        "## <font color = 'indianred'>**Training Loop**"
      ],
      "id": "Cw1DGXWpD0Wn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eJAKrRoD0Wn"
      },
      "source": [
        "**Model Training** involves five steps:\n",
        "\n",
        "- Step 0: Randomly initialize parameters / weights\n",
        "- Step 1: Compute model's predictions - forward pass\n",
        "- Step 2: Compute loss\n",
        "- Step 3: Compute the gradients\n",
        "- Step 4: Update the parameters\n",
        "- Step 5: Repeat steps 1 - 4\n",
        "\n",
        "Model training is repeating this process over and over, for many **epochs**.\n",
        "\n",
        "We will specify number of ***epochs*** and during each epoch we will iterate over the complete dataset and will keep on updating the parameters.\n",
        "\n",
        "***Learning rate*** and ***epochs*** are known as hyperparameters. We have to adjust the values of these two based on validation dataset.\n",
        "\n",
        "We will now create functions for step 1 to 4."
      ],
      "id": "0eJAKrRoD0Wn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Installed the torchmetrics package, a PyTorch library for various machine learning metrics,\n",
        "# to facilitate model evaluation during and after training.\n",
        "!pip install torchmetrics\n"
      ],
      "metadata": {
        "id": "R8EcSFdVFYkt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dd527c6-0605-449c-93b4-6fac8851f2c0"
      },
      "id": "R8EcSFdVFYkt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
            "\u001b[?25l     \u001b[90m\u001b[0m \u001b[32m0.0/840.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m\u001b[0m\u001b[90m\u001b[0m\u001b[90m\u001b[0m \u001b[32m194.6/840.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m\u001b[0m \u001b[32m840.4/840.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.10.1 torchmetrics-1.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03x_-TFpD0Wn"
      },
      "outputs": [],
      "source": [
        "# Imported HammingDistance from torchmetrics\n",
        "# HammingDistance is useful for evaluating multi-label classification problems.\n",
        "from torchmetrics import HammingDistance"
      ],
      "id": "03x_-TFpD0Wn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color = 'indianred'>**Hamming Distance**</font> is often used in multi-label classification problems to quantify the dissimilarity between the predicted and true labels. It does this by measuring the number of label positions where predicted and true labels differ for each sample. It is a useful metric because it offers a granular level of understanding of the discrepancies between the predicted and actual labels, taking into account each label in a multi-label setting.\n",
        "\n",
        "<font color = 'indianred'>**Unlike accuracy, which is all-or-nothing, Hamming Distance can give partial credit by considering the labels that were correctly classified** </font>, thereby providing a more granular insight into the model's performance.\n",
        "\n",
        "Let us understand this with an example:"
      ],
      "metadata": {
        "id": "aJrHO7YNW_4z"
      },
      "id": "aJrHO7YNW_4z"
    },
    {
      "cell_type": "code",
      "source": [
        "target = torch.tensor([[0, 1], [1, 1]])\n",
        "preds = torch.tensor([[0, 1], [0, 1]])\n",
        "hamming_distance = HammingDistance(task=\"multilabel\", num_labels=2)\n",
        "hamming_distance(preds, target)"
      ],
      "metadata": {
        "id": "H9_bbYVRVgar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec38293-f4d5-42ab-9a43-71c3a5016fd9"
      },
      "id": "H9_bbYVRVgar",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.2500)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the given example, the Hamming Distance is calculated for multi-label classification with two labels (0 and 1).\n",
        "\n",
        "1. The target tensor has shape (2, 2): `[[0, 1], [1, 1]]`\n",
        "2. The prediction tensor also has shape (2, 2): `[[0, 1], [0, 1]]`\n",
        "\n",
        "Let's examine the individual sample pairs to understand the distance:\n",
        "\n",
        "- For the first sample pair (target = `[0, 1]`, prediction = `[0, 1]`), the Hamming Distance is 0 because the prediction is accurate.\n",
        "- For the second sample pair (target = `[1, 1]`, prediction = `[0, 1]`), the Hamming Distance is 1 for the first label (predicted 0, true label 1).\n",
        "\n",
        "To calculate the overall Hamming Distance, we can take the number of label mismatches and divide by the total number of labels:\n",
        "\n",
        "- Total Mismatches = 1 (from the second sample pair)\n",
        "- Total Number of Labels = 2 samples * 2 labels per sample = 4\n",
        "\n",
        "Therefore, the overall Hamming Distance is \\(1 / 4 = 0.25\\), which matches the output `tensor(0.2500)`.\n",
        "\n",
        "Hamming Distance is a good metric for multi-label classification as it can capture the difference between sets of labels per sample, thereby providing a more granular measure of the model's performance."
      ],
      "metadata": {
        "id": "QQvqbLYeV2oL"
      },
      "id": "QQvqbLYeV2oL"
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epochs, loss_function, learning_rate, model, optimizer, train_loader, device):\n",
        "\n",
        "    train_hamming_distance = HammingDistance(task=\"multilabel\", num_labels=3).to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Initialize train_loss at the start of the epoch\n",
        "        running_train_loss = 0.0\n",
        "\n",
        "        # Iterate on batches from the dataset using train_loader\n",
        "        for x, y in train_loader:\n",
        "            # Move inputs and outputs to GPUs\n",
        "            x = x.to(device, dtype=torch.float32)\n",
        "            y = y.to(device,dtype=torch.float32)\n",
        "\n",
        "            # Step 1: Forward Pass: Compute model's predictions\n",
        "            output =  model(x)\n",
        "\n",
        "            # Step 2: Compute loss\n",
        "            loss =  loss_function(output,y)\n",
        "\n",
        "            # Step 3: Backward pass - Compute the gradients\n",
        "            # Zero out gradients from the previous iteration\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Backward pass: Compute gradients based on the loss\n",
        "            loss.backward()\n",
        "\n",
        "            # Step 4: Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update running loss\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Correct prediction using thresholding\n",
        "                y_pred = output > 0.5\n",
        "\n",
        "                # Update Hamming Distance metric\n",
        "                train_hamming_distance.update(y_pred, y)\n",
        "\n",
        "        # Compute mean train loss for the epoch\n",
        "        train_loss = running_train_loss / len(train_loader)\n",
        "\n",
        "        # Compute Hamming Distance for the epoch\n",
        "        epoch_hamming_distance = train_hamming_distance.compute()\n",
        "\n",
        "        # Print the train loss and Hamming Distance for the epoch\n",
        "        print(f'Epoch: {epoch + 1} / {epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Train Hamming Distance: {epoch_hamming_distance:.4f}')\n",
        "\n",
        "        # Reset metric states for the next epoch\n",
        "        train_hamming_distance.reset()\n"
      ],
      "metadata": {
        "id": "MOvMeCtlULhs"
      },
      "id": "MOvMeCtlULhs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a manual seed for reproducibility across runs\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# Define hyperparameters: learning rate and the number of epochs\n",
        "learning_rate = 1\n",
        "epochs = 20\n",
        "\n",
        "# Determine the computing device (GPU if available, otherwise CPU)\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Student Task: Configure the optimizer for model training.\n",
        "# Here, we're using Stochastic Gradient Descent (SGD). Think through what parameters are needed.\n",
        "# Reminder: Utilize the learning rate defined above when setting up your optimizer.\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Transfer the model to the selected device (CPU or GPU)\n",
        "model.to(device)\n",
        "\n",
        "# Apply custom weight initialization function to the model layers\n",
        "# Note: Weight initialization can significantly affect training dynamics\n",
        "model.apply(init_weights)\n",
        "\n",
        "# Call the training function to start the training process\n",
        "# Note: All elements like epochs, loss function, learning rate, etc., are passed as arguments\n",
        "train(epochs, loss_function, learning_rate, model, optimizer, train_loader, device)\n"
      ],
      "metadata": {
        "id": "GvVE1B4DD0Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600eb2f0-ba79-41f7-89de-935b87f9604b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Epoch: 1 / 20\n",
            "Train Loss: 0.5126 | Train Hamming Distance: 0.2947\n",
            "Epoch: 2 / 20\n",
            "Train Loss: 0.4856 | Train Hamming Distance: 0.2523\n",
            "Epoch: 3 / 20\n",
            "Train Loss: 0.4825 | Train Hamming Distance: 0.2547\n",
            "Epoch: 4 / 20\n",
            "Train Loss: 0.4833 | Train Hamming Distance: 0.2540\n",
            "Epoch: 5 / 20\n",
            "Train Loss: 0.4866 | Train Hamming Distance: 0.2530\n",
            "Epoch: 6 / 20\n",
            "Train Loss: 0.4829 | Train Hamming Distance: 0.2563\n",
            "Epoch: 7 / 20\n",
            "Train Loss: 0.4856 | Train Hamming Distance: 0.2520\n",
            "Epoch: 8 / 20\n",
            "Train Loss: 0.4843 | Train Hamming Distance: 0.2553\n",
            "Epoch: 9 / 20\n",
            "Train Loss: 0.4858 | Train Hamming Distance: 0.2533\n",
            "Epoch: 10 / 20\n",
            "Train Loss: 0.4860 | Train Hamming Distance: 0.2540\n",
            "Epoch: 11 / 20\n",
            "Train Loss: 0.4846 | Train Hamming Distance: 0.2620\n",
            "Epoch: 12 / 20\n",
            "Train Loss: 0.4842 | Train Hamming Distance: 0.2510\n",
            "Epoch: 13 / 20\n",
            "Train Loss: 0.4845 | Train Hamming Distance: 0.2563\n",
            "Epoch: 14 / 20\n",
            "Train Loss: 0.4848 | Train Hamming Distance: 0.2523\n",
            "Epoch: 15 / 20\n",
            "Train Loss: 0.4833 | Train Hamming Distance: 0.2527\n",
            "Epoch: 16 / 20\n",
            "Train Loss: 0.4846 | Train Hamming Distance: 0.2593\n",
            "Epoch: 17 / 20\n",
            "Train Loss: 0.4847 | Train Hamming Distance: 0.2533\n",
            "Epoch: 18 / 20\n",
            "Train Loss: 0.4854 | Train Hamming Distance: 0.2533\n",
            "Epoch: 19 / 20\n",
            "Train Loss: 0.4828 | Train Hamming Distance: 0.2560\n",
            "Epoch: 20 / 20\n",
            "Train Loss: 0.4822 | Train Hamming Distance: 0.2507\n"
          ]
        }
      ],
      "id": "GvVE1B4DD0Wn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iT6TOxnXD0Wn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9b9c6a-a248-4caf-819a-7bb33e9565f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "weight tensor([[ 0.9856, -0.1141, -0.2715,  0.0869, -0.9284],\n",
            "        [-0.9591,  0.7973,  0.5119,  0.1237, -1.4677],\n",
            "        [ 0.1281,  0.7948, -0.0565, -1.6264,  0.5559]])\n",
            "bias tensor([-0.2102,  0.4204,  0.0613])\n"
          ]
        }
      ],
      "source": [
        "# Loop through the model's parameters to display them\n",
        "# This is helpful for debugging and understanding how well the model has learned\n",
        "for name, param in model.named_parameters():\n",
        "    # 'name' will contain the name of the parameter (e.g., 'layer1.weight')\n",
        "    # 'param.data' will contain the parameter values\n",
        "    print(name, param.data)\n"
      ],
      "id": "iT6TOxnXD0Wn"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}